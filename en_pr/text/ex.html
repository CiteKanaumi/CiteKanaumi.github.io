The problem under study: variable selection in functional regression
The study of regression models is clearly among the leading topics in statistics. In particular, these models play a central role in the theory of statistics with functional data, often called Functional Data Analysis (FDA); see [7,15,16] for an overview on FDA.
Throughout this paper, we will consider ‘‘functional data’’ consisting of independent X1 = X1(t),...,Xn = Xn(t) observations (trajectories) drawn from a second-order (L2 ) stochastic process X = X (t ), t ∈ [0, 1], with continuous trajectories and continuous mean and covariance functions, denoted by m = m(t ) and K (s, t ), respectively. All the involved random variables are supposed to be defined in a common probability space (Ω , A, Pr).
We are interested on functional regression models with scalar response, of type Yi = g(Xi) + εi, where g is a real function defined on a suitable space X where the trajectories of our process are supposed to live. The random variables εi are independent errors (and also independent from the Xi ) with mean zero and common variance σ 2 .
More specifically, we are concerned with variable selection issues; see, [4, Sec. 1], [11] for additional information and references. Basically, a variable selection functional method is an automatic procedure that takes a function {x(t ), t ∈ [0, 1]} to a finite-dimensional vector (x(t1),...,x(tp)). The overall idea of variable selection is to choose the variables x(ti) (or, equivalently,the‘‘impactpoints’’t1,...,tp ∈[0,1];see[22]),inan‘‘optimalway’’sothattheoriginalfunctionalproblem (regression, classification, clustering, ...) is replaced with the corresponding multivariate version, based on the selected variables. In the regression setting, this would amount to replace the functional model Yi = g(Xi) + εi with a finite dimensional version of type Yi =φ{Xi(t1),...,Xi(tp)}+ei.
Nevertheless,note that still the problem is of a functional nature,
since the methods to select the ti are generally based upon the full data trajectories.
<br>
Some notation
A set of possible ‘‘impact’’ points t1 , . . . , tp ∈ [0, 1] will be denoted T (sometimes S ) or Tp when we want to stress the
cardinality of T . Also, X (Tp ) will stand for (X (t1 ), . . . , X (tp ))⊤ . The superindex ∗ will be used to denote that the points ti∗ are the ‘‘true’’ ones, or the ‘‘optimal’’ ones according to some criterion.
<br>
Given a random variable Z(with finite variance)the notation Z Tp will refer to the orthogonal projection of Z on the space spanned by the components of X(Tp) − m(Tp).
If p∗ < p, the notation Tp∗ ≺ Tp will indicate that all the points in Tp∗ belong also to Tp.
Finally, as usual in statistics, we use an upper hat sign to denote the estimated quantities (or the predicted variables). For instance, Tp will denote a data-driven estimator of Tp and YˆT will stand for the corresponding (fully data-driven) prediction
of the response Y Tp.
The halfway notation YˆTp will represent the orthogonal projection of the responsevariable onto the space spanned by the marginal variables indexed by the estimated points T_p.
<br>
Some motivation. The drawbacks of the classical linear L2-model for variable selection purposes
It is quite natural to assume that the explanatory functional variables Xi = Xi(t) are members of the space L2[0,1],
endowed with the usual inner product ⟨x1,x2⟩2 = ∫1 x1(t)x2(t)dt, for x1,x2 ∈ L2[0,1]. In this setting, the most popular 0
choice for g is, by far, a linear (or affine) operator from L2 [0, 1] to R which leads to a model of type

<br>Yi =α0+⟨Xi,β⟩2+εi,i=1,...,n,
<br>
where X=X(t) is the explanatory functional variable, α0 ∈R is the intercept constant and β∈L2[0,1] denotes the slope function. As in the standard multivariate regression model, the aim here is to estimate α0 and β in order to be able to make accurate predictions of the response variable Y .
The corresponding theory is outlined in several places; see, e.g., the article [6] or the books [13,16]. The Hilbert structure of the L2 [0, 1] space allows us to keep ourselves as close as possible to the usual least squares framework in multivariate regression; for example, the projection P(x) of an element x on a closed subspace H is characterized by the orthogonality condition ⟨x − P(x),a⟩2 = 0, for all a ∈ H. However, other crucial differences with the finite-dimensional case (mostly associated with the non-invertibility of the covariance operator of the process X (t )) make the functional L2 regression theory far from trivial. Most of these difficulties are intrinsic to the infinite-dimensional nature of the data, so that they cannot be overcome by just replacing L2 [0, 1] with another function space. However, when it comes to variable selection applied to linear regression, it would be useful to have the finite dimensional linear model (based on the selected variables)
Yi =α0 +∑βjXi(tj)+εi, i=1,...,n
<br>
as a particular case of our general model. Notice that (2) cannot be established in the L2 framework, since a transformation of type x ∈ L2[0, 1] ↦→ ∑pi=1βix(tj) is not a linear continuous functional in L2. In heuristic terms, one would need to look for the slope function β in a suitable space, for which a ‘‘finite-dimensional’’ model such as (2) could make sense. More precisely, we will change the ‘‘habitat’’ space for the function β : instead of assuming β ∈ L2 [0, 1] we will assume that β belongs to the Reproducing Kernel Hilbert Space (RKHS), H(K ), associated with K .
As we will see below, the assumed membership of β to H(K) entails some additional restrictions of regularity on the slope function β (when compared to the simple assumption β ∈ L2 [0, 1]). In any case, such a situation is not unusual: some restrictions on β appear in different ways even when the classical L2-model (1) is considered. The reason is that the space L2 [0, 1] is in fact too large from several points of view. Hence, in spite of the advantages of the L2 -model commented above, one typically uses penalization or projection methods to exclude extremely rough solutions in the estimation of β.
Our proposal here, as presented in the next section, aims at reconciling two targets: first, we look for a functional linear model, wide enough to include finite-dimensional versions, such as (2), as particular cases. Second, we would like to achieve such a goal with a minimal change in the space where β lives.
<br><br>
Some related literature
A quite general RKHS-based approach to the problem of dimension reduction in functional regression has been proposed by [18]. These authors follow the inverse regression methodology to deal with a model of type Y = l(ξ1 , . . . ξd ) + ε where l is a link function and the ξj are linear functionals of the explanatory variable X, defined in RKHS terms. This pioneering reference shows very clearly the huge potential of the RKHS approach. However, as the authors point out, there are still many aspects not considered in that paper and worth of attention. Variable selection is one of them. In fact, the whole point of the present paper is to show that things become particularly simple when the RKHS machinery is applied to variable selection. A recent use of RKHS methods in the problem of functional binary classification is developed in [5]. See also [3,17] for a broader perspective of the applicability of RKHS methods in statistics.
<br>
Other variable selection methods, always aimed at selecting the ‘‘best points’’ t1 , . . . , td (or the ‘‘best variables’’ X (t1 ), . . . , X (tp )) have been proposed as well, with no explicit reference of RKHS tools. Thus, the selection of the ‘‘best impact point’’ t1 in a model of type (2) with p = 1 is addressed in [25]. Different variable selection methods have been suggested by [1,9,12] for prediction and classification purposes. In addition, a non-parametric approach for non-linear functional regression models is presented in [2]. See the references therein for more information on non-linear models. Also, a criterion for ‘‘optimal design’’ in trajectory recovery is considered in [20].
<br>
A recent general proposal for dimension reduction (beyond variable selection and regression models) is [14].
Organization of the paper
In Section 2 we introduce and motivate (in population terms) our variable selection procedure. The asymptotic properties of the empirical version (when the parameters are estimated) are considered in Section 3. The problems associated with the choice of the number p of selected variables are analyzed in Sections 4 and 5. The empirical results (simulations and real data examples) are presented in Section 6. Section 7 includes some final comments and conclusions. The proofs of some results (stated but not proved in the previous sections) are included in Section 8.
